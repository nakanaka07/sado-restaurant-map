# optimized_places_data_updater.py
import requests
import gspread
import time
import os
import json
import re
from typing import List, Dict, Tuple, Optional
from dotenv import load_dotenv

# æ”¹å–„ã•ã‚ŒãŸæ¤œç´¢æˆ¦ç•¥ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from improved_search_strategy import ImprovedSearchStrategy, SearchResultAnalyzer
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    class ImprovedSearchStrategy:
        def should_skip_search(self, query): return False
        def generate_smart_search_queries(self, query, category): return [f"{query} ä½æ¸¡"] if "ä½æ¸¡" not in query else [query]
        def optimize_search_order(self, queries): return queries
    class SearchResultAnalyzer:
        def analyze_successful_query(self, *args): pass
        def analyze_failed_query(self, *args): pass
        def get_recommendations(self): return {}

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
if os.path.exists('.env'):
    load_dotenv()

# --- è¨­å®šé …ç›® ---
SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))

# GitHub Actionsç’°å¢ƒã§ã®ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
PLACES_API_KEY = os.environ.get('PLACES_API_KEY')
SPREADSHEET_ID = os.environ.get('SPREADSHEET_ID')

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®ãƒ‘ã‚¹è¨­å®š
if 'GOOGLE_SERVICE_ACCOUNT_KEY' in os.environ:
    # GitHub Actions: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰JSONã‚­ãƒ¼ã‚’å–å¾—
    SERVICE_ACCOUNT_FILE_PATH = os.path.join(SCRIPT_DIR, 'your-service-account-key.json')
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º: æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
    SERVICE_ACCOUNT_FILE_PATH = os.environ.get('GOOGLE_SERVICE_ACCOUNT_PATH', 
                                               os.path.join(SCRIPT_DIR, 'your-service-account-key.json'))

# APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰- ç’°å¢ƒå¤‰æ•°ã§èª¿æ•´å¯èƒ½
API_REQUEST_DELAY = float(os.environ.get('API_DELAY', '1'))
SHEETS_API_DELAY = 1.5

# æ›´æ–°å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æŒ‡å®šï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼‰
TARGET_DATA = os.environ.get('TARGET_DATA', 'all')  # 'all', 'restaurants', 'parkings', 'toilets'

# ä½æ¸¡å³¶ã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹
SADO_BOUNDS = {
    "north": 38.4,
    "south": 37.7,
    "east": 138.6,
    "west": 138.0
}

# ä½æ¸¡ã®åœ°åŒºåˆ†é¡ï¼ˆä½æ¸¡å¸‚å…¬å¼ã‚µã‚¤ãƒˆåŸºæº–ï¼‰
# å‚è€ƒ: https://www.city.sado.niigata.jp/soshiki/2002/2359.html
SADO_DISTRICTS = {
    'ä¸¡æ´¥åœ°åŒº': [
        'ä¸¡æ´¥', 'æ²³å´', 'ç§‹æ´¥', 'æ¢…æ´¥', 'æ¹Š', 'åŸé»’', 'åŒ—ç”°é‡æµ¦', 'æ˜¥æ—¥', 'æµœç”°', 'åŠ èŒ‚æ­Œä»£',
        'ç¾½å‰', 'æ¤¿', 'åŒ—äº”åé‡Œ', 'ç™½ç€¬', 'ç‰å´', 'å’Œæœ¨', 'é¦¬é¦–', 'åŒ—æ¾ã‚±å´', 'å¹³æ¾',
        'æµ¦å·', 'æ­Œè¦‹', 'é»’å§«', 'è™«å´', 'ä¸¡æ´¥å¤§å·', 'ç¾½äºŒç”Ÿ', 'ä¸¡å°¾', 'æ¤æ³Š', 'çœŸæœ¨',
        'ä¸‹ä¹…çŸ¥', 'ä¹…çŸ¥æ²³å†…', 'åŸè…°', 'ä½å‰', 'å¾æ½Ÿ', 'ç«‹é‡', 'ä¸Šæ¨ªå±±', 'é•·æ±Ÿ',
        'æ½Ÿç«¯', 'ä¸‹æ¨ªå±±', 'æ—­', 'æ°´æ´¥', 'ç‰‡é‡å°¾', 'æœˆå¸ƒæ–½', 'é‡æµ¦', 'æ±å¼·æ¸…æ°´',
        'æ±ç«‹å³¶', 'å²©é¦–', 'æ±éµœå³¶', 'æŸ¿é‡æµ¦', 'è±Šå²¡', 'ç«‹é–“', 'èµ¤ç‰', 'èš«',
        'åŒ—å°æµ¦', 'è¦‹ç«‹', 'é·²å´', 'é¡˜', 'åŒ—éµœå³¶', 'çœŸæ›´å·', 'ä¸¡æ´¥ç¦æµ¦', 'è—»æµ¦'
    ],
    'ç›¸å·åœ°åŒº': [
        'ç›¸å·', 'æˆ¸ä¸­', 'åŒ—ç«‹å³¶', 'é”è€…', 'å…¥å·', 'åŒ—ç‰‡è¾º', 'é–¢', 'é«˜ç€¬', 'æ©˜', 'ç¨²é¯¨',
        'ç±³éƒ·', 'äºŒè¦‹', 'ä¸‹ç›¸å·', 'å°å·', 'å§«æ´¥', 'åŒ—ç‹„', 'æˆ¸åœ°', 'å—ç‰‡è¾º', 'çŸ³èŠ±',
        'å¾Œå°¾', 'åŒ—å·å†…', 'é«˜åƒ', 'å°é‡è¦‹', 'çŸ³å', 'å°ç”°', 'å¤§å€‰', 'çŸ¢æŸ„',
        'äº”åæµ¦', 'å²©è°·å£', 'ç›¸å·å¤§æµ¦',
        # ç›¸å·ç”ºå†…ã®è©³ç´°åœ°åã‚‚å«ã‚ã‚‹
        'ç›¸å·æ°´é‡‘ç”º', 'ç›¸å·æŸ´ç”º', 'ç›¸å·å¤§é–“ç”º', 'ç›¸å·ç´™å±‹ç”º', 'ç›¸å·ç‚­å±‹ç”º', 'ç›¸å·æ¿å·ç”º',
        'ç›¸å·å‚ä¸‹ç”º', 'ç›¸å·åŒ—æ²¢ç”º', 'ç›¸å·ä¸‹å±±ä¹‹ç¥ç”º', 'ç›¸å·æŸ„æ“ç”º', 'ç›¸å·å¥ˆè‰¯ç”º', 'å¥ˆè‰¯ç”º',
        'ç›¸å·å˜‰å·¦è¡›é–€ç”º', 'ç›¸å·æ¸…å³è¡›é–€ç”º', 'ç›¸å·éŠ€å±±ç”º', 'ç›¸å·å°å³è¡›é–€ç”º', 'ç›¸å·å‹˜å››éƒç”º',
        'ä¸Šç›¸å·ç”º', 'ç›¸å·äº”éƒå³è¡›é–€ç”º', 'ç›¸å·å®—å¾³ç”º', 'ç›¸å·åº„å³è¡›é–€ç”º', 'ç›¸å·æ¬¡åŠ©ç”º',
        'ç›¸å·è«è¨ªç”º', 'ç›¸å·å¤§å·¥ç”º', 'ç›¸å·æ–°äº”éƒç”º', 'ç›¸å·å…­å³è¡›é–€ç”º', 'ç›¸å·ä¸Šäº¬ç”º',
        'ç›¸å·å·¦é–€ç”º', 'ç›¸å·å¤§åºŠå±‹ç”º', 'ç›¸å·ä¸­äº¬ç”º', 'ç›¸å·ä¸‹äº¬ç”º', 'ç›¸å·å…«ç™¾å±‹ç”º',
        'ç›¸å·ä¼šæ´¥ç”º', 'ç›¸å·å‘³å™Œå±‹ç”º', 'ç›¸å·ç±³å±‹ç”º', 'ç›¸å·å¤•ç™½ç”º', 'ç›¸å·å¼¥åéƒç”º',
        'ç›¸å·å››åç‰©ç”º', 'ç›¸å·åºƒé–“ç”º', 'ç›¸å·è¥¿å‚ç”º', 'ç›¸å·é•·å‚ç”º', 'ç›¸å·ä¸Šå¯ºç”º',
        'ç›¸å·ä¸­å¯ºç”º', 'ç›¸å·ä¸‹å¯ºç”º', 'ç›¸å·å—æ²¢ç”º', 'ç›¸å·å°å…­ç”º', 'ç›¸å·æ–°è¥¿å‚ç”º',
        'ç›¸å·çŸ³æ‰£ç”º', 'ç›¸å·å¡©å±‹ç”º', 'ç›¸å·æ¿ç”º', 'ç›¸å·ææœ¨ç”º', 'ç›¸å·æ–°ææœ¨ç”º',
        'ç›¸å·ç¾½ç”°ç”º', 'ç›¸å·æ±Ÿæˆ¸æ²¢ç”º', 'ç›¸å·ä¸€ç”ºç›®', 'ç›¸å·ä¸€ç”ºç›®è£ç”º', 'ç›¸å·ä¸€ç”ºç›®æµœç”º',
        'ç›¸å·äºŒç”ºç›®', 'ç›¸å·äº”éƒå·¦è¡›é–€ç”º', 'ç›¸å·äºŒç”ºç›®æµœç”º', 'ç›¸å·äºŒç”ºç›®æ–°æµœç”º',
        'ç›¸å·ä¸‰ç”ºç›®', 'ç›¸å·ä¸‰ç”ºç›®æµœç”º', 'ç›¸å·ä¸‰ç”ºç›®æ–°æµœç”º', 'ç›¸å·å››ç”ºç›®',
        'ç›¸å·å››ç”ºç›®æµœç”º', 'ç›¸å·å¸‚ç”º', 'ç›¸å·æ–°æµœç”º', 'ç›¸å·é¦¬ç”º', 'ç›¸å·ç¾½ç”°æ‘',
        'ç›¸å·ä¸‹æˆ¸ç”º', 'ç›¸å·ä¸‹æˆ¸æµœç”º', 'ç›¸å·ä¸‹æˆ¸ç‚­å±‹ç”º', 'ç›¸å·ä¸‹æˆ¸ç‚­å±‹è£ç”º',
        'ç›¸å·ä¸‹æˆ¸ç‚­å±‹æµœç”º', 'ç›¸å·æµ·å£«ç”º', 'ç›¸å·ä¸‹æˆ¸æ‘', 'ç›¸å·é¹¿ä¼', 'ç›¸å·æ „ç”º'
    ],
    'ä½å’Œç”°åœ°åŒº': [
        'ä½å’Œç”°', 'æ²¢æ ¹', 'çªªç”°', 'ä¸­åŸ', 'æ²³åŸç”°', 'å…«å¹¡', 'å…«å¹¡æ–°ç”º', 'å…«å¹¡ç”º',
        'æ²³åŸç”°æœ¬ç”º', 'æ²³åŸç”°è«è¨ªç”º', 'é›å†¶ç”º', 'çŸ³ç”°', 'ä¸Šé•·æœ¨', 'ä¸‹é•·æœ¨', 'é•·æœ¨',
        'ä¸ŠçŸ¢é¦³', 'äºŒå®®', 'å¸‚é‡æ²¢', 'çœŸå…‰å¯º', 'å±±ç”°', 'é’é‡', 'æ±å¤§é€š',
        'æ²¢æ ¹äº”åé‡Œ', 'æ²¢æ ¹ç¯­ç”º', 'æ²¢æ ¹ç‚­å±‹ç”º', 'æ²¢æ ¹ç”º'
    ],
    'é‡‘äº•åœ°åŒº': [
        'é‡‘äº•', 'åƒç¨®', 'å‰äº•', 'æ³‰', 'ä¸­èˆˆ', 'å¹³æ¸…æ°´', 'é‡‘äº•æ–°ä¿', 'è²å¡š',
        'å¤§å’Œ', 'å‰äº•æœ¬éƒ·', 'å®‰é¤Šå¯º', 'ä¸‰ç€¬å·', 'æ°´æ¸¡ç”°'
    ],
    'æ–°ç©‚åœ°åŒº': [
        'æ–°ç©‚', 'æ–°ç©—', 'å¤§é‡', 'ä¸‹æ–°ç©‚', 'èˆŸä¸‹', 'çš†å·', 'æ–°ç©‚çš†å·', 'æ–°ç©‚èˆŸä¸‹', 'æ–°ç©‚æ­¦äº•',
        'æ–°ç©‚å¤§é‡', 'æ–°ç©‚äº•å†…', 'ä¸Šæ–°ç©‚', 'æ–°ç©‚ç“œç”Ÿå±‹', 'æ–°ç©‚æ­£æ˜å¯º', 'æ–°ç©‚ç”°é‡æ²¢',
        'æ–°ç©‚æ½Ÿä¸Š', 'æ–°ç©‚é’æœ¨', 'æ–°ç©‚é•·ç•', 'æ–°ç©‚åŒ—æ–¹'
    ],
    'ç•‘é‡åœ°åŒº': [
        'ç•‘é‡', 'å®®å·', 'æ —é‡æ±Ÿ', 'ç›®é»’ç”º', 'ä¸‰å®®', 'æ¾ã‚±å´', 'å¤šç”°', 'å¯ºç”°', 'é£¯æŒ',
        'ç•‰ç”°', 'å¤§ä¹…ä¿', 'çŒ¿å…«', 'å°å€‰', 'é•·è°·', 'åŠã‚±æµ¦', 'æµœæ²³å†…', 'ä¸¸å±±'
    ],
    'çœŸé‡åœ°åŒº': [
        'çœŸé‡', 'è±Šç”°', 'å››æ—¥ç”º', 'å‰å²¡', 'å¤§å°', 'é‡‘ä¸¸', 'é•·çŸ³', 'çœŸé‡æ–°ç”º', 'æ»è„‡',
        'èƒŒåˆ', 'å¤§é ˆ', 'é™å¹³', 'ä¸‹é»’å±±', 'çœŸé‡å¤§å·', 'åå¤å±‹', 'å›½åˆ†å¯º', 'ç«¹ç”°',
        'é˜¿ä»åŠ', 'é˜¿ä½›åŠ', 'å¤§å€‰è°·', 'ç”°åˆ‡é ˆ', 'è¥¿ä¸‰å·', 'æ¤¿å°¾'
    ],
    'å°æœ¨åœ°åŒº': [
        'å°æœ¨', 'å®¿æ ¹æœ¨', 'æ·±æµ¦', 'ç”°é‡æµ¦', 'å¼·æ¸…æ°´', 'å°æœ¨ç”º', 'å°æœ¨æœ¨é‡æµ¦', 'å°æ¯”å¡',
        'å°æœ¨å ‚é‡œ', 'äº•åª', 'å°æœ¨å¤§æµ¦', 'æœ¨æµ', 'æ±Ÿç©', 'æ²¢å´', 'çŠ¬ç¥å¹³',
        'å°æœ¨å¼·æ¸…æ°´', 'ç´æµ¦', 'å°æœ¨é‡‘ç”°æ–°ç”°'
    ],
    'ç¾½èŒ‚åœ°åŒº': [
        'ç¾½èŒ‚', 'å¤§çŸ³', 'ç¾½èŒ‚æœ¬éƒ·', 'ç¾½èŒ‚ä¸‰ç€¬', 'äº€è„‡', 'ç¾½èŒ‚æ»å¹³', 'ç¾½èŒ‚å¤§å´',
        'ç¾½èŒ‚é£¯å²¡', 'ç¾½èŒ‚ä¸Šå±±ç”°', 'ç¾½èŒ‚å¤§æ©‹', 'ç¾½èŒ‚å¤§çŸ³', 'ç¾½èŒ‚æ‘å±±', 'ç¾½èŒ‚äº€è„‡',
        'ç¾½èŒ‚å°æ³Š'
    ],
    'èµ¤æ³Šåœ°åŒº': [
        'èµ¤æ³Š', 'å¾³å’Œ', 'æŸ³æ²¢', 'èšå ´', 'å¤§æ‰', 'æ‰é‡æµ¦', 'å—æ–°ä¿', 'çœŸæµ¦', 'ä¸‰å·',
        'å¤–å±±', 'ä¸Šå·èŒ‚', 'ä¸‹å·èŒ‚'
    ]
}

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
BASE_FIELDS = [
    "places.id",
    "places.shortFormattedAddress", 
    "places.location",
    "places.displayName",
    "places.primaryType",
    "places.primaryTypeDisplayName",
    "places.googleMapsLinks"
]

# é£²é£Ÿåº—ç”¨ã®è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
RESTAURANT_ADDITIONAL_FIELDS = [
    "places.regularOpeningHours",
    "places.nationalPhoneNumber",
    "places.rating",
    "places.userRatingCount"
]

# ã‚«ãƒ†ã‚´ãƒªè¨­å®š
CATEGORIES = {
    "é£²é£Ÿåº—": {
        "worksheet": "é£²é£Ÿåº—",
        "worksheet_outside": "é£²é£Ÿåº—_ä½æ¸¡å¸‚å¤–",
        "fields": BASE_FIELDS + RESTAURANT_ADDITIONAL_FIELDS,
        "included_types": ["restaurant", "meal_takeaway", "cafe", "bar", "bakery"],
        "search_terms": ["ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³", "é£Ÿå ‚", "ã‚«ãƒ•ã‚§", "å±…é…’å±‹"]
    },
    "é§è»Šå ´": {
        "worksheet": "é§è»Šå ´", 
        "worksheet_outside": "é§è»Šå ´_ä½æ¸¡å¸‚å¤–",
        "fields": BASE_FIELDS,
        "included_types": ["parking"],
        "search_terms": ["é§è»Šå ´", "ãƒ‘ãƒ¼ã‚­ãƒ³ã‚°", "parking"]
    },
    "å…¬è¡†ãƒˆã‚¤ãƒ¬": {
        "worksheet": "å…¬è¡†ãƒˆã‚¤ãƒ¬",
        "worksheet_outside": "å…¬è¡†ãƒˆã‚¤ãƒ¬_ä½æ¸¡å¸‚å¤–",
        "fields": BASE_FIELDS,
        "included_types": [],
        "search_terms": ["ãƒˆã‚¤ãƒ¬", "å…¬è¡†ãƒˆã‚¤ãƒ¬", "ä¾¿æ‰€", "åŒ–ç²§å®¤"]
    }
}

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
# çµ±ä¸€ãƒ˜ãƒƒãƒ€ãƒ¼å®šç¾©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config.headers import get_unified_header, get_not_found_header, UNIFIED_HEADERS

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€å¾“æ¥ã®å¤‰æ•°åã‚‚ä¿æŒ
HEADERS = UNIFIED_HEADERS
NOT_FOUND_HEADERS = get_not_found_header()

def normalize_address(address):
    """ä½æ¸¡ã®ä½æ‰€ã‚’æ­£è¦åŒ–"""
    replacements = {
        'æ–°æ½ŸçœŒä½æ¸¡å¸‚': 'ä½æ¸¡å¸‚',
        'ä½æ¸¡éƒ¡': 'ä½æ¸¡å¸‚',
        'ä¸¡æ´¥å¸‚': 'ä½æ¸¡å¸‚ä¸¡æ´¥',
        'ç›¸å·ç”º': 'ä½æ¸¡å¸‚ç›¸å·',
        'ä½å’Œç”°ç”º': 'ä½æ¸¡å¸‚ä½å’Œç”°',
        'é‡‘äº•ç”º': 'ä½æ¸¡å¸‚é‡‘äº•',
        'æ–°ç©‚æ‘': 'ä½æ¸¡å¸‚æ–°ç©‚',
        'ç•‘é‡ç”º': 'ä½æ¸¡å¸‚ç•‘é‡',
        'çœŸé‡ç”º': 'ä½æ¸¡å¸‚çœŸé‡',
        'å°æœ¨ç”º': 'ä½æ¸¡å¸‚å°æœ¨',
        'ç¾½èŒ‚ç”º': 'ä½æ¸¡å¸‚ç¾½èŒ‚',
        'èµ¤æ³Šæ‘': 'ä½æ¸¡å¸‚èµ¤æ³Š'
    }
    
    normalized = address
    for old, new in replacements.items():
        normalized = normalized.replace(old, new)
    
    return normalized

def is_within_sado_bounds(lat, lng):
    """ç·¯åº¦çµŒåº¦ãŒä½æ¸¡å³¶ã®å¢ƒç•Œå†…ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    try:
        lat = float(lat)
        lng = float(lng)
    except (ValueError, TypeError):
        return False
    
    return (SADO_BOUNDS["south"] <= lat <= SADO_BOUNDS["north"] and 
            SADO_BOUNDS["west"] <= lng <= SADO_BOUNDS["east"])

def classify_district_by_coordinates(lat, lng):
    """ç·¯åº¦çµŒåº¦ã«ã‚ˆã‚‹åœ°åŒºåˆ¤å®šï¼ˆè£œåŠ©æ©Ÿèƒ½ï¼‰"""
    try:
        lat = float(lat)
        lng = float(lng)
    except (ValueError, TypeError):
        return None
    
    # ä½æ¸¡å³¶ã®å¢ƒç•Œå¤–ã®å ´åˆã¯é™¤å¤–
    if not is_within_sado_bounds(lat, lng):
        return None
    
    # ä½æ¸¡å³¶ã®å„åœ°åŒºã®å¤§ã¾ã‹ãªå¢ƒç•Œï¼ˆç·¯åº¦çµŒåº¦ç¯„å›²ï¼‰
    district_bounds = {
        'ä¸¡æ´¥åœ°åŒº': {
            'lat_min': 37.95, 'lat_max': 38.45, 
            'lng_min': 138.35, 'lng_max': 138.60
        },
        'ç›¸å·åœ°åŒº': {
            'lat_min': 38.00, 'lat_max': 38.25,
            'lng_min': 138.20, 'lng_max': 138.50
        },
        'ä½å’Œç”°åœ°åŒº': {
            'lat_min': 37.90, 'lat_max': 38.10,
            'lng_min': 138.30, 'lng_max': 138.45
        },
        'é‡‘äº•åœ°åŒº': {
            'lat_min': 37.85, 'lat_max': 38.05,
            'lng_min': 138.25, 'lng_max': 138.40
        },
        'æ–°ç©‚åœ°åŒº': {
            'lat_min': 37.88, 'lat_max': 38.08,
            'lng_min': 138.20, 'lng_max': 138.35
        },
        'ç•‘é‡åœ°åŒº': {
            'lat_min': 37.82, 'lat_max': 38.00,
            'lng_min': 138.35, 'lng_max': 138.50
        },
        'çœŸé‡åœ°åŒº': {
            'lat_min': 37.90, 'lat_max': 38.05,
            'lng_min': 138.15, 'lng_max': 138.35
        },
        'å°æœ¨åœ°åŒº': {
            'lat_min': 37.80, 'lat_max': 37.95,
            'lng_min': 138.25, 'lng_max': 138.40
        },
        'ç¾½èŒ‚åœ°åŒº': {
            'lat_min': 37.75, 'lat_max': 37.90,
            'lng_min': 138.30, 'lng_max': 138.45
        },
        'èµ¤æ³Šåœ°åŒº': {
            'lat_min': 37.70, 'lat_max': 37.85,
            'lng_min': 138.35, 'lng_max': 138.50
        }
    }
    
    # å„åœ°åŒºã®å¢ƒç•Œã‚’ãƒã‚§ãƒƒã‚¯
    for district, bounds in district_bounds.items():
        if (bounds['lat_min'] <= lat <= bounds['lat_max'] and 
            bounds['lng_min'] <= lng <= bounds['lng_max']):
            return district
    
    return None

def classify_district(address):
    """ä½æ‰€ã‹ã‚‰åœ°åŒºã‚’åˆ†é¡ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    normalized_address = normalize_address(address)
    
    # ã‚ˆã‚Šç²¾å¯†ãªãƒãƒƒãƒãƒ³ã‚°ï¼ˆéƒ¨åˆ†ä¸€è‡´ã‹ã‚‰å®Œå…¨ä¸€è‡´å„ªå…ˆã¸ï¼‰
    for district, areas in SADO_DISTRICTS.items():
        for area in areas:
            if area in normalized_address:
                return district
    
    # ã€Œãã®ä»–ã€ã«ãªã£ãŸå ´åˆã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    print(f"âš ï¸  åœ°åŒºåˆ†é¡ã€Œãã®ä»–ã€: {normalized_address}")
    return "ãã®ä»–"

def generate_search_queries(query, category):
    """æ”¹å–„ã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ"""
    strategy = ImprovedSearchStrategy()
    
    # æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãã‚¯ã‚¨ãƒªã‹ãƒã‚§ãƒƒã‚¯
    if strategy.should_skip_search(query):
        print(f"  â­ï¸ Skipping query (contains exclusion keywords): {query}")
        return []
    
    # ã‚¹ãƒãƒ¼ãƒˆãªæ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
    queries = strategy.generate_smart_search_queries(query, category)
    
    # æ¤œç´¢é †åºã®æœ€é©åŒ–
    optimized_queries = strategy.optimize_search_order(queries)
    
    return optimized_queries

def search_places_multi_pattern(query, category):
    """æ”¹å–„ã•ã‚ŒãŸæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å ´æ‰€ã‚’æ¤œç´¢ã™ã‚‹"""
    search_queries = generate_search_queries(query, category)
    
    # æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆ
    if not search_queries:
        return 'SKIPPED', [], query
    
    analyzer = SearchResultAnalyzer()
    
    for i, search_query in enumerate(search_queries):
        print(f"  Trying pattern {i+1}/{len(search_queries)}: '{search_query}'")
        
        status, places = search_places_new_api(search_query, category)
        
        if status == 'OK' and places:
            print(f"  âœ“ Found {len(places)} places with pattern {i+1}")
            # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
            analyzer.analyze_successful_query(query, search_query, category)
            return status, places, search_query
        
        if status == 'REQUEST_FAILED':
            return status, [], search_query
            
        # çŸ­ã„å¾…æ©Ÿæ™‚é–“
        if i < len(search_queries) - 1:
            time.sleep(0.5)
    
    # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
    analyzer.analyze_failed_query(query, category)
    print(f"  âœ— No results found with any pattern")
    return 'ZERO_RESULTS', [], search_queries[0] if search_queries else query

def search_places_new_api(text_query, category):
    """æ–°ã—ã„Places APIï¼ˆText Searchï¼‰ã‚’ä½¿ç”¨ã—ã¦å ´æ‰€ã‚’æ¤œç´¢"""
    if not PLACES_API_KEY:
        print("PLACES_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return 'SKIPPED', []
    
    config = CATEGORIES[category]
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‚’æ§‹ç¯‰
    request_body = {
        "textQuery": text_query,
        "languageCode": "ja",
        "maxResultCount": 20,
        "locationBias": {
            "rectangle": {
                "low": {"latitude": SADO_BOUNDS["south"], "longitude": SADO_BOUNDS["west"]},
                "high": {"latitude": SADO_BOUNDS["north"], "longitude": SADO_BOUNDS["east"]}
            }
        }
    }
    
    # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ 
    if config["included_types"]:
        request_body["includedType"] = config["included_types"][0]
    
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': PLACES_API_KEY,
        'X-Goog-FieldMask': ','.join(config["fields"])
    }
    
    try:
        response = requests.post(
            'https://places.googleapis.com/v1/places:searchText',
            headers=headers,
            json=request_body
        )
        response.raise_for_status()
        data = response.json()
        
        places = data.get('places', [])
        return 'OK' if places else 'ZERO_RESULTS', places
        
    except requests.exceptions.RequestException as e:
        print(f"API request failed: {e}")
        return 'REQUEST_FAILED', []

def get_place_details_from_cid(cid_url, category):
    """
    CID URLã‹ã‚‰Placeè©³ç´°ã‚’ç›´æ¥å–å¾—
    
    Args:
        cid_url: Google Maps CID URL (ä¾‹: https://maps.google.com/place?cid=1234567890)
        category: ã‚«ãƒ†ã‚´ãƒªå
    
    Returns:
        tuple: (status, places_list)
    """
    if not PLACES_API_KEY:
        print("PLACES_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return 'SKIPPED', []
    
    # CIDã‚’Place IDã«å¤‰æ›ï¼ˆç°¡ç•¥åŒ–ï¼šCIDã‚’ãã®ã¾ã¾place_idã¨ã—ã¦ä½¿ç”¨ï¼‰
    # å®Ÿéš›ã®CID->Place IDå¤‰æ›ã¯è¤‡é›‘ãªãŸã‚ã€ã“ã“ã§ã¯CIDã‚’ç›´æ¥ä½¿ç”¨
    import re
    cid_match = re.search(r'cid=(\d+)', cid_url)
    if not cid_match:
        print(f"âŒ CID URLã®å½¢å¼ãŒä¸æ­£: {cid_url}")
        return 'INVALID_CID', []
    
    cid = cid_match.group(1)
    
    # CIDã‹ã‚‰Place IDã‚’å–å¾—ã™ã‚‹ç°¡æ˜“çš„ãªæ–¹æ³•
    # Google Maps CIDã¯ãã®ã¾ã¾Place IDã¨ã—ã¦ä½¿ãˆãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€
    # å®Ÿéš›ã«ã¯Text Searchã‚’ä½¿ç”¨ã—ã¦CIDã«å¯¾å¿œã™ã‚‹Placeã‚’æ¤œç´¢
    try:
        # CIDã‚’ä½¿ã£ãŸURLæ¤œç´¢ï¼ˆä»£æ›¿æ‰‹æ³•ï¼‰
        # Note: CIDç›´æ¥æ¤œç´¢ã¯ Places API (New) ã§ã¯éå¯¾å¿œã®ãŸã‚ã€
        # å°†æ¥çš„ã«ã¯Legacy Places APIã¾ãŸã¯åˆ¥ã®æ‰‹æ³•ãŒå¿…è¦
        print(f"ğŸ” CID URLæ¤œç´¢: {cid}")
        return search_places_new_api(f"place_id:{cid}", category)
        
    except Exception as e:
        print(f"âŒ CIDæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return 'CID_SEARCH_FAILED', []

def process_query(query, category):
    """
    ã‚¯ã‚¨ãƒªã‚’å‡¦ç†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã¾ãŸã¯CID URLï¼‰
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆåº—èˆ—åã¾ãŸã¯CID URLï¼‰
        category: ã‚«ãƒ†ã‚´ãƒªå
    
    Returns:
        tuple: (status, places_list, processed_query)
    """
    if 'cid=' in query:
        # CID URL ã®å ´åˆ
        print(f"ğŸ†” CID URLå‡¦ç†: {query}")
        status, places = get_place_details_from_cid(query, category)
        return status, places, query
    else:
        # ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã®å ´åˆï¼ˆå¾“æ¥é€šã‚Šï¼‰
        return search_places_multi_pattern(query, category)

def determine_category_from_place(place):
    """å ´æ‰€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    name = place.get('displayName', {}).get('text', '').lower()
    address = place.get('shortFormattedAddress', '').lower()
    primary_type = place.get('primaryType', '')
    
    # åå‰ã¨ä½æ‰€ã«ã‚ˆã‚‹åˆ¤å®šï¼ˆå„ªå…ˆï¼‰
    toilet_keywords = ["ãƒˆã‚¤ãƒ¬", "ä¾¿æ‰€", "åŒ–ç²§å®¤", "restroom", "toilet"]
    parking_keywords = ["é§è»Š", "parking", "ãƒ‘ãƒ¼ã‚­ãƒ³ã‚°"]
    
    if any(keyword in name for keyword in toilet_keywords) or any(keyword in address for keyword in toilet_keywords):
        return "å…¬è¡†ãƒˆã‚¤ãƒ¬"
    
    if any(keyword in name for keyword in parking_keywords) or any(keyword in address for keyword in parking_keywords):
        return "é§è»Šå ´"
    
    # ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹åˆ¤å®š
    if primary_type == "parking":
        return "é§è»Šå ´"
    elif primary_type in ["restaurant", "meal_takeaway", "cafe", "bar", "bakery", "food"]:
        return "é£²é£Ÿåº—"
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é£²é£Ÿåº—
    return "é£²é£Ÿåº—"

def format_detailed_opening_hours(opening_hours_data):
    """å–¶æ¥­æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not opening_hours_data:
        return ""
    
    # weekdayTextãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
    weekday_text = opening_hours_data.get('weekdayText', [])
    if weekday_text:
        return "\n".join(weekday_text)
    
    # periodsã‹ã‚‰æ§‹ç¯‰
    periods = opening_hours_data.get('periods', [])
    if not periods:
        return ""
    
    formatted_hours = []
    days = ['æ—¥', 'æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ']
    
    for period in periods:
        open_time = period.get('open', {})
        close_time = period.get('close', {})
        
        day = open_time.get('day', 0)
        day_name = days[day] if 0 <= day < 7 else str(day)
        
        open_hour = f"{open_time.get('hour', 0):02d}:{open_time.get('minute', 0):02d}"
        
        if close_time:
            close_hour = f"{close_time.get('hour', 0):02d}:{close_time.get('minute', 0):02d}"
            formatted_hours.append(f"{day_name}: {open_hour}-{close_hour}")
        else:
            formatted_hours.append(f"{day_name}: {open_hour}-24:00")
    
    return "\n".join(formatted_hours)

def extract_place_data(place, category):
    """å ´æ‰€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    place_id = place.get('id', '')
    name = place.get('displayName', {}).get('text', '')
    raw_address = place.get('shortFormattedAddress', '')
    address = normalize_address(raw_address)
    
    location = place.get('location', {})
    lat = location.get('latitude', '')
    lng = location.get('longitude', '')
    
    primary_type = place.get('primaryType', '')
    primary_type_display = place.get('primaryTypeDisplayName', {}).get('text', '')
    
    # åœ°åŒºåˆ†é¡ï¼ˆæ”¹è‰¯ç‰ˆï¼šç·¯åº¦çµŒåº¦ã‚’å„ªå…ˆã—ãŸä½æ¸¡å³¶å†…åˆ¤å®šï¼‰
    district = classify_district(address)
    
    # ä½æ‰€ã«ã‚ˆã‚‹åˆ¤å®šãŒã€Œãã®ä»–ã€ã®å ´åˆã€ç·¯åº¦çµŒåº¦ã«ã‚ˆã‚‹åˆ¤å®šã‚’è©¦è¡Œ
    if district == 'ãã®ä»–' and lat and lng:
        coord_district = classify_district_by_coordinates(lat, lng)
        if coord_district:
            district = coord_district
            print(f"  ğŸ“ ç·¯åº¦çµŒåº¦ã«ã‚ˆã‚Š {district} ã«åˆ†é¡: {name}")
    
    # Google Mapsãƒªãƒ³ã‚¯ã¯previewãªã®ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯URLã‚’ç”Ÿæˆ
    google_maps_url = ""
    if 'googleMapsLinks' in place and place['googleMapsLinks']:
        google_maps_url = place['googleMapsLinks'].get('directionsLink', 
                         place['googleMapsLinks'].get('searchLink', ''))
    
    if not google_maps_url and place_id:
        google_maps_url = f"https://www.google.com/maps/search/?api=1&query=Google&query_place_id={place_id}"
    
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’æœ€é©åŒ–
    if category == "é£²é£Ÿåº—":
        phone = place.get('nationalPhoneNumber', '')
        opening_hours = format_detailed_opening_hours(place.get('regularOpeningHours'))
        rating = place.get('rating', '')
        review_count = place.get('userRatingCount', '')
        
        return [
            place_id, name, address, lat, lng,
            primary_type, primary_type_display, phone, opening_hours,
            rating, review_count, district, google_maps_url, timestamp
        ]
    
    else:
        # é§è»Šå ´ãƒ»å…¬è¡†ãƒˆã‚¤ãƒ¬: ã‚·ãƒ³ãƒ—ãƒ«æ§‹æˆ
        return [
            place_id, name, address, lat, lng,
            primary_type, primary_type_display, district, google_maps_url, timestamp
        ]

def is_within_sado(place, category):
    """å ´æ‰€ãŒä½æ¸¡å³¶å†…ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆç·¯åº¦çµŒåº¦ã‚’å„ªå…ˆï¼‰"""
    location = place.get('location', {})
    lat = location.get('latitude', '')
    lng = location.get('longitude', '')
    
    # ç·¯åº¦çµŒåº¦ã«ã‚ˆã‚‹åˆ¤å®šï¼ˆæœ€å„ªå…ˆï¼‰
    if lat and lng:
        if is_within_sado_bounds(lat, lng):
            return True
        else:
            return False
    
    # ç·¯åº¦çµŒåº¦ãŒãªã„å ´åˆã¯ä½æ‰€ã«ã‚ˆã‚‹åˆ¤å®š
    raw_address = place.get('shortFormattedAddress', '')
    address = normalize_address(raw_address)
    
    # ä½æ¸¡é–¢é€£ã®ä½æ‰€è¡¨è¨˜ã‚’ãƒã‚§ãƒƒã‚¯
    sado_keywords = ['ä½æ¸¡å¸‚', 'ä½æ¸¡', 'æ–°æ½ŸçœŒä½æ¸¡', 'ä¸¡æ´¥', 'ç›¸å·', 'ä½å’Œç”°', 'é‡‘äº•', 'æ–°ç©‚', 'ç•‘é‡', 'çœŸé‡', 'å°æœ¨', 'ç¾½èŒ‚', 'èµ¤æ³Š']
    
    return any(keyword in address for keyword in sado_keywords)

def get_or_create_worksheet(spreadsheet, worksheet_name):
    """ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
    try:
        return spreadsheet.worksheet(worksheet_name)
    except gspread.exceptions.WorksheetNotFound:
        print(f"Creating worksheet: {worksheet_name}")
        return spreadsheet.add_worksheet(title=worksheet_name, rows=1000, cols=20)

def update_worksheet(spreadsheet, category, places_data):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ï¼ˆæ”¹è‰¯ç‰ˆï¼šä½æ¸¡å³¶å†…å¤–ã®æŒ¯ã‚Šåˆ†ã‘å¯¾å¿œï¼‰"""
    
    # ä½æ¸¡å³¶å†…å¤–ã§ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¯ã‚Šåˆ†ã‘
    sado_places = []
    outside_places = []
    
    for place in places_data:
        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
        place_category = determine_category_from_place(place)
        if place_category != category:
            continue
            
        place_id = place.get('id', '')
        if not place_id:
            continue
        
        # ä½æ¸¡å³¶å†…å¤–ã‚’åˆ¤å®š
        if is_within_sado(place, category):
            sado_places.append(place)
        else:
            outside_places.append(place)
    
    # ä½æ¸¡å³¶å†…ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
    if sado_places:
        worksheet_name = CATEGORIES[category]["worksheet"]
        print(f"Updating worksheet: {worksheet_name} ({len(sado_places)} places)")
        _update_single_worksheet(spreadsheet, category, worksheet_name, sado_places, is_sado=True)
    
    # ä½æ¸¡å³¶å¤–ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
    if outside_places:
        worksheet_name_outside = CATEGORIES[category]["worksheet_outside"]
        print(f"Updating worksheet: {worksheet_name_outside} ({len(outside_places)} places)")
        _update_single_worksheet(spreadsheet, category, worksheet_name_outside, outside_places, is_sado=False)

def _update_single_worksheet(spreadsheet, category, worksheet_name, places_data, is_sado=True):
    """å˜ä¸€ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã®æ›´æ–°å‡¦ç†"""
    worksheet = get_or_create_worksheet(spreadsheet, worksheet_name)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèªãƒ»è¨­å®š
    try:
        existing_headers = worksheet.row_values(1)
    except:
        existing_headers = []
    
    expected_headers = HEADERS[category]
    if existing_headers != expected_headers:
        worksheet.update('A1', [expected_headers])
        print(f"Headers updated for {worksheet_name}")
        all_records = []
    else:
        try:
            all_records = worksheet.get_all_records(expected_headers=expected_headers)
        except:
            # ãƒ˜ãƒƒãƒ€ãƒ¼é‡è¤‡ã®å ´åˆã€å¼·åˆ¶çš„ã«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°
            worksheet.update('A1', [expected_headers])
            print(f"Headers forcefully updated for {worksheet_name} due to duplicates")
            all_records = []
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ—ä½œæˆ
    existing_data = {
        record['ãƒ—ãƒ¬ã‚¤ã‚¹ID']: {'data': record, 'row': i + 2}
        for i, record in enumerate(all_records) 
        if record.get('ãƒ—ãƒ¬ã‚¤ã‚¹ID')
    }
    
    updates = []
    appends = []
    
    for place in places_data:
        place_id = place.get('id', '')
        if not place_id:
            continue
            
        row_data = extract_place_data(place, category)
        
        # ä½æ¸¡å³¶å¤–ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€åœ°åŒºæƒ…å ±ã‚’ã€Œå¸‚å¤–ã€ã«è¨­å®š
        if not is_sado:
            if category == "é£²é£Ÿåº—":
                row_data[11] = "å¸‚å¤–"  # åœ°åŒºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            else:
                row_data[7] = "å¸‚å¤–"   # åœ°åŒºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        
        if place_id in existing_data:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°åˆ¤å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            old_data = existing_data[place_id]['data']
            row_num = existing_data[place_id]['row']
            
            # ã‚ˆã‚Šè©³ç´°ãªæ›´æ–°åˆ¤å®š
            address_field = "ä½æ‰€" if category == "é£²é£Ÿåº—" else "æ‰€åœ¨åœ°"
            needs_update = (
                str(old_data.get(address_field, '')) != str(row_data[2]) or
                str(old_data.get('åœ°åŒº', '')) != str(row_data[-3]) or  # åœ°åŒºæƒ…å ±
                (category == "é£²é£Ÿåº—" and (
                    str(old_data.get('é›»è©±ç•ªå·', '')) != str(row_data[7]) or
                    str(old_data.get('å–¶æ¥­æ™‚é–“', '')) != str(row_data[8])
                ))
            )
            
            if needs_update:
                end_col = chr(ord('A') + len(expected_headers) - 1)
                updates.append({
                    'range': f'A{row_num}:{end_col}{row_num}',
                    'values': [row_data]
                })
        else:
            appends.append(row_data)
    
    # ãƒãƒƒãƒæ›´æ–°
    if updates:
        worksheet.batch_update(updates)
        print(f"Updated {len(updates)} rows in {worksheet_name}")
        time.sleep(SHEETS_API_DELAY)
    
    if appends:
        worksheet.append_rows(appends)
        print(f"Added {len(appends)} new rows in {worksheet_name}")

def update_not_found_worksheet(spreadsheet, not_found_data):
    """æ¤œç´¢çµæœãªã—ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ï¼ˆæ”¹è‰¯ç‰ˆ - åŠ¹ç‡åŒ–å¯¾å¿œï¼‰"""
    if not not_found_data:
        return
    
    worksheet = get_or_create_worksheet(spreadsheet, "æ¤œç´¢çµæœãªã—")
    
    try:
        existing_headers = worksheet.row_values(1)
    except:
        existing_headers = []
    
    # NOT_FOUND_HEADERSã‚’ä½¿ç”¨ï¼ˆæ—¢ã«ã€Œã‚¹ã‚­ãƒƒãƒ—ç†ç”±ã€ã¨ã€Œæ”¹å–„ææ¡ˆã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ï¼‰
    expected_headers = NOT_FOUND_HEADERS
    
    if existing_headers != expected_headers:
        worksheet.update('A1', [expected_headers])
        all_records = []
    else:
        try:
            all_records = worksheet.get_all_records(expected_headers=expected_headers)
        except Exception as e:
            print(f"âš ï¸ Error reading worksheet {worksheet.title}: {str(e)}")
            # ãƒ˜ãƒƒãƒ€ãƒ¼é‡è¤‡ã®å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å†è¨­å®š
            if "duplicates" in str(e).lower():
                worksheet.update('A1', [expected_headers])
                print(f"Headers reset for {worksheet.title} due to duplicates")
            all_records = []
    
    # æ—¢å­˜ã‚¯ã‚¨ãƒªãƒãƒƒãƒ—
    existing_queries = {
        (record['æ¤œç´¢èª'], record['ã‚«ãƒ†ã‚´ãƒª']): {'data': record, 'row': i + 2}
        for i, record in enumerate(all_records)
        if record.get('æ¤œç´¢èª') and record.get('ã‚«ãƒ†ã‚´ãƒª')
    }
    
    current_date = time.strftime('%Y-%m-%d')
    updates = []
    appends = []
    
    for item in not_found_data:
        query = item['original_query']
        search_query = item['search_query']
        category = item['category']
        skip_reason = item.get('skip_reason', '')
        suggestion = item.get('suggestion', '')
        
        key = (query, category)
        
        if key in existing_queries:
            old_data = existing_queries[key]['data']
            if old_data.get('æ¤œç´¢æ—¥') == current_date:
                # è©¦è¡Œå›æ•°ã‚’å¢—ã‚„ã™
                attempts = int(old_data.get('è©¦è¡Œå›æ•°', 1)) + 1
                row_num = existing_queries[key]['row']
                updates.append({
                    'range': f'A{row_num}:G{row_num}',
                    'values': [[query, search_query, current_date, attempts, category, skip_reason, suggestion]]
                })
            else:
                appends.append([query, search_query, current_date, 1, category, skip_reason, suggestion])
        else:
            appends.append([query, search_query, current_date, 1, category, skip_reason, suggestion])
    
    if updates:
        worksheet.batch_update(updates)
        print(f"Updated {len(updates)} not-found entries")
    if appends:
        worksheet.append_rows(appends)
        print(f"Added {len(appends)} new not-found entries")

def authenticate_google_sheets():
    """Google Sheetsèªè¨¼"""
    creds_json_str = os.environ.get('GOOGLE_SERVICE_ACCOUNT_KEY')
    
    try:
        if creds_json_str:
            creds_dict = json.loads(creds_json_str)
            return gspread.service_account_from_dict(creds_dict)
        else:
            return gspread.service_account(filename=SERVICE_ACCOUNT_FILE_PATH)
    except Exception as e:
        print(f"Googleèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_queries_from_file(filename):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¯ã‚¨ãƒªã‚’èª­ã¿è¾¼ã¿ï¼ˆURLå¯¾å¿œç‰ˆï¼‰
    
    å¯¾å¿œå½¢å¼:
    - åº—èˆ—åï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
    - Google Maps URL (cid=XXXXX)
    """
    file_path = os.path.join(SCRIPT_DIR, filename)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            queries = []
            for line in f:
                line = line.strip()
                # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¨ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if not line or line.startswith('#'):
                    continue
                
                # URL ã®å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                if 'cid=' in line and '#' in line:
                    url_part = line.split('#')[0].strip()
                    queries.append(url_part)
                else:
                    queries.append(line)
        
        # URL ã¨ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®çµ±è¨ˆ
        url_count = sum(1 for q in queries if 'cid=' in q)
        text_count = len(queries) - url_count
        
        print(f"ğŸ“Š {filename}ã‹ã‚‰èª­ã¿è¾¼ã¿: ç·è¨ˆ{len(queries)}ä»¶ (URL: {url_count}ä»¶, ãƒ†ã‚­ã‚¹ãƒˆ: {text_count}ä»¶)")
        return queries
    except FileNotFoundError:
        print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}")
        return []

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
    if not PLACES_API_KEY or not SPREADSHEET_ID:
        print("Error: PLACES_API_KEY or SPREADSHEET_ID not set")
        return
    
    print(f"ğŸ¯ Target data: {TARGET_DATA}")
    print(f"â±ï¸ API delay: {API_REQUEST_DELAY}s")
    
    # Google Sheetsèªè¨¼
    gc = authenticate_google_sheets()
    if not gc:
        return
    
    try:
        spreadsheet = gc.open_by_key(SPREADSHEET_ID)
    except Exception as e:
        print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆTARGET_DATAã«åŸºã¥ãé¸æŠçš„å‡¦ç†ï¼‰
    query_files = {
        'é£²é£Ÿåº—': 'data/urls/restaurants_merged.txt',  # ğŸ†• çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        'å…¬è¡†ãƒˆã‚¤ãƒ¬': 'data/queries/toilets.txt', 
        'é§è»Šå ´': 'data/queries/parkings.txt'
    }
    
    # TARGET_DATAã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿
    if TARGET_DATA != 'all':
        category_mapping = {
            'restaurants': 'é£²é£Ÿåº—',
            'toilets': 'å…¬è¡†ãƒˆã‚¤ãƒ¬', 
            'parkings': 'é§è»Šå ´'
        }
        
        if TARGET_DATA in category_mapping:
            category = category_mapping[TARGET_DATA]
            query_files = {category: query_files[category]}
            print(f"ğŸ“ Processing only: {category}")
        else:
            print(f"âŒ Unknown target data: {TARGET_DATA}")
            return
    
    all_not_found = []
    total_api_calls = 0
    
    for category, filename in query_files.items():
        queries = load_queries_from_file(filename)
        if not queries:
            continue
        
        print(f"\n=== Processing {category} ({len(queries)} queries) ===")
        all_places = []
        category_not_found = []
        
        for i, query in enumerate(queries, 1):
            print(f"[{i}/{len(queries)}] Processing: {query}")
            
            status, places, search_query = process_query(query, category)
            
            if status == 'SKIPPED':
                # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®å‡¦ç†
                category_not_found.append({
                    'original_query': query,
                    'search_query': search_query,
                    'category': category,
                    'skip_reason': 'é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º',
                    'suggestion': 'åº—èˆ—åã‚’ç¢ºèªã—ã€ç¾åœ¨ã®æ­£å¼åç§°ã§å†æ¤œç´¢ã—ã¦ãã ã•ã„'
                })
                continue
            
            # APIå‘¼ã³å‡ºã—å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã‚¹ã‚­ãƒƒãƒ—æ™‚ã¯é™¤å¤–ï¼‰
            search_queries = generate_search_queries(query, category)
            total_api_calls += len(search_queries) if search_queries else 0
            
            if status == 'ZERO_RESULTS':
                category_not_found.append({
                    'original_query': query,
                    'search_query': search_query,
                    'category': category,
                    'skip_reason': '',
                    'suggestion': 'åœ°åŸŸåã‚„æ¥­ç¨®ã‚’è¿½åŠ ã—ãŸæ¤œç´¢ã‚’è©¦ã—ã¦ãã ã•ã„'
                })
            elif status == 'OK':
                all_places.extend(places)
            elif status == 'REQUEST_FAILED':
                print(f"  âš ï¸ API request failed for: {query}")
                break
            
            time.sleep(API_REQUEST_DELAY)
        
        # é‡è¤‡é™¤å»
        unique_places = list({place['id']: place for place in all_places 
                            if place.get('id')}.values())
        
        print(f"Found {len(unique_places)} unique places for {category}")
        
        # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆæ›´æ–°
        if unique_places:
            update_worksheet(spreadsheet, category, unique_places)
        
        all_not_found.extend(category_not_found)
        time.sleep(SHEETS_API_DELAY)
    
    # æ¤œç´¢çµæœãªã—ã®è¨˜éŒ²
    if all_not_found:
        update_not_found_worksheet(spreadsheet, all_not_found)
        print(f"\n{len(all_not_found)} queries had no results")
    
    # ã‚³ã‚¹ãƒˆè¨ˆç®—ã¨åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆ
    cost_per_request = 0.017  # Text Search (New) ã®æ–™é‡‘ (USD)
    estimated_cost = total_api_calls * cost_per_request
    
    # åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    total_queries = sum(len(load_queries_from_file(query_files[cat])) for cat in query_files.keys())
    skipped_queries = sum(1 for item in all_not_found if item.get('skip_reason'))
    success_rate = ((total_queries - len(all_not_found)) / total_queries * 100) if total_queries > 0 else 0
    
    print(f"\nâœ… Process completed!")
    print(f"ğŸ“Š Processing Statistics:")
    print(f"   - Total queries processed: {total_queries}")
    print(f"   - Queries with results: {total_queries - len(all_not_found)}")
    print(f"   - Queries skipped (optimization): {skipped_queries}")
    print(f"   - Queries with no results: {len(all_not_found) - skipped_queries}")
    print(f"   - Success rate: {success_rate:.1f}%")
    print(f"ğŸ’° API Usage:")
    print(f"   - Total API calls made: {total_api_calls}")
    print(f"   - Estimated cost: ${estimated_cost:.3f} USD")
    print(f"   - Average calls per query: {total_api_calls/total_queries:.1f}" if total_queries > 0 else "")
    print(f"ğŸ“… Target data: {TARGET_DATA}")
    
    # ã‚¹ã‚­ãƒƒãƒ—ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ
    if skipped_queries > 0:
        saved_cost = skipped_queries * cost_per_request
        print(f"ğŸ’¡ Cost savings from smart skipping: ${saved_cost:.3f} USD ({skipped_queries} queries)")

if __name__ == '__main__':
    main()