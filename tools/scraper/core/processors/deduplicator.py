#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿é‡è¤‡é™¤å»å‡¦ç†ã‚¯ãƒ©ã‚¹
CIDã¨ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢é …ç›®ã®é‡è¤‡æ’é™¤å‡¦ç†

æ©Ÿèƒ½:
- CIDä»˜ãæ–½è¨­ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡é™¤å»
- ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢é …ç›®ã®é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
- ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆæ™‚ã®é‡è¤‡æ’é™¤
- æ–½è¨­åã®æ­£è¦åŒ–ãƒ»æ¯”è¼ƒå‡¦ç†

ä½¿ç”¨ä¾‹:
    from core.processors.deduplicator import DataDeduplicator
    deduplicator = DataDeduplicator()
    deduplicator.remove_duplicates('data/urls/restaurants_merged.txt')
"""

import re
import urllib.parse
from pathlib import Path
from typing import Set, List, Tuple
from difflib import SequenceMatcher

def normalize_facility_name(name: str) -> str:
    """
    æ–½è¨­åã‚’æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒç”¨ï¼‰

    Args:
        name: æ–½è¨­å

    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸæ–½è¨­å
    """
    # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»ã—ã€çµ±ä¸€åŒ–
    normalized = name.lower()
    normalized = re.sub(r'[+ï¼‹\s\-ï¼ãƒ»]', '', normalized)
    normalized = re.sub(r'é§è»Šå ´$', '', normalized)
    normalized = re.sub(r'å…¬è¡†ãƒˆã‚¤ãƒ¬$', '', normalized)
    normalized = re.sub(r'ãƒˆã‚¤ãƒ¬$', '', normalized)
    return normalized

def similarity(a: str, b: str) -> float:
    """
    æ–‡å­—åˆ—ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—

    Args:
        a, b: æ¯”è¼ƒã™ã‚‹æ–‡å­—åˆ—

    Returns:
        é¡ä¼¼åº¦ (0.0-1.0)
    """
    return SequenceMatcher(None, a, b).ratio()

def extract_cid_facilities(file_path: Path) -> List[Tuple[str, str]]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CIDä»˜ãæ–½è¨­ã‚’æŠ½å‡º

    Args:
        file_path: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        (CID, æ–½è¨­å)ã®ãƒªã‚¹ãƒˆ
    """
    cid_facilities = []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lines = content.split('\n')

        for line in lines:
            line = line.strip()

            # CIDä»˜ãURLã‚’æ¤œç´¢
            if 'cid=' in line:
                cid_match = re.search(r'cid=(\d+)', line)
                comment_match = re.search(r'#\s*(.+)$', line)

                if cid_match and comment_match:
                    cid = cid_match.group(1)
                    facility_name = comment_match.group(1).strip()
                    cid_facilities.append((cid, facility_name))

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path.name} - {e}")

    return cid_facilities

def extract_text_facilities(file_path: Path) -> List[str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢é …ç›®ã‚’æŠ½å‡º

    Args:
        file_path: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢é …ç›®ã®ãƒªã‚¹ãƒˆ
    """
    text_facilities = []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lines = content.split('\n')
        in_text_section = False

        for line in lines:
            line = line.strip()

            # ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹å§‹ã‚’æ¤œå‡º
            if 'ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢' in line:
                in_text_section = True
                continue

            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã§ã€ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚„ç©ºè¡Œä»¥å¤–ã‚’åé›†
            if in_text_section:
                if line and not line.startswith('#') and not line.startswith('='):
                    text_facilities.append(line)

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path.name} - {e}")

    return text_facilities

def find_duplicates(cid_facilities: List[Tuple[str, str]], text_facilities: List[str]) -> Tuple[List[str], List[Tuple[str, str, float]]]:
    """
    CIDã¨ãƒ†ã‚­ã‚¹ãƒˆé …ç›®ã®é‡è¤‡ã‚’æ¤œå‡º

    Args:
        cid_facilities: (CID, æ–½è¨­å)ã®ãƒªã‚¹ãƒˆ
        text_facilities: ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢é …ç›®ã®ãƒªã‚¹ãƒˆ

    Returns:
        (å®Œå…¨é‡è¤‡ãƒªã‚¹ãƒˆ, é¡ä¼¼é‡è¤‡ãƒªã‚¹ãƒˆ)
    """
    exact_duplicates = []
    similar_duplicates = []

    # CIDæ–½è¨­åã®æ­£è¦åŒ–ãƒãƒƒãƒ—ã‚’ä½œæˆ
    cid_normalized = {}
    for cid, name in cid_facilities:
        normalized = normalize_facility_name(name)
        cid_normalized[normalized] = (cid, name)

    for text_item in text_facilities:
        text_normalized = normalize_facility_name(text_item)

        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if text_normalized in cid_normalized:
            exact_duplicates.append(text_item)
        else:
            # é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
            for cid_norm, (cid, cid_name) in cid_normalized.items():
                sim = similarity(text_normalized, cid_norm)
                if sim >= 0.8:  # 80%ä»¥ä¸Šã®é¡ä¼¼åº¦
                    similar_duplicates.append((text_item, cid_name, sim))

    return exact_duplicates, similar_duplicates

def remove_duplicates_from_file(file_path: Path, duplicates_to_remove: List[str]) -> int:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é‡è¤‡é …ç›®ã‚’å‰Šé™¤

    Args:
        file_path: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
        duplicates_to_remove: å‰Šé™¤ã™ã‚‹é …ç›®ã®ãƒªã‚¹ãƒˆ

    Returns:
        å‰Šé™¤ã•ã‚ŒãŸé …ç›®æ•°
    """
    if not duplicates_to_remove:
        return 0

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        new_lines = []
        removed_count = 0
        in_text_section = False

        for line in lines:
            original_line = line.rstrip('\n')
            stripped_line = line.strip()

            # ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œå‡º
            if 'ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢' in stripped_line:
                in_text_section = True
                new_lines.append(original_line)
                continue

            # ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã§é‡è¤‡é …ç›®ã‚’ãƒã‚§ãƒƒã‚¯
            if in_text_section and stripped_line in duplicates_to_remove:
                removed_count += 1
                print(f"  ğŸ—‘ï¸  å‰Šé™¤: {stripped_line}")
                continue

            new_lines.append(original_line)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
        with open(file_path, 'w', encoding='utf-8') as f:
            for line in new_lines:
                f.write(line + '\n')

        return removed_count

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path.name} - {e}")
        return 0

def process_file(file_path: Path) -> None:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡æ’é™¤ã‚’å®Ÿè¡Œ

    Args:
        file_path: å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
    """
    print(f"\nğŸ” å‡¦ç†ä¸­: {file_path.name}")

    # CIDã¨ãƒ†ã‚­ã‚¹ãƒˆé …ç›®ã‚’æŠ½å‡º
    cid_facilities = extract_cid_facilities(file_path)
    text_facilities = extract_text_facilities(file_path)

    print(f"  ğŸ“Š CIDä»˜ãæ–½è¨­: {len(cid_facilities)}ä»¶")
    print(f"  ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢é …ç›®: {len(text_facilities)}ä»¶")

    # é‡è¤‡ã‚’æ¤œå‡º
    exact_duplicates, similar_duplicates = find_duplicates(cid_facilities, text_facilities)

    if exact_duplicates:
        print(f"  âš ï¸  å®Œå…¨é‡è¤‡: {len(exact_duplicates)}ä»¶")
        for item in exact_duplicates:
            print(f"    - {item}")

    if similar_duplicates:
        print(f"  âš ï¸  é¡ä¼¼é‡è¤‡: {len(similar_duplicates)}ä»¶")
        for text_item, cid_name, sim in similar_duplicates:
            print(f"    - {text_item} â‰ˆ {cid_name} ({sim:.2f})")

    # é‡è¤‡ã‚’å‰Šé™¤
    all_duplicates = exact_duplicates + [item[0] for item in similar_duplicates]

    if all_duplicates:
        removed_count = remove_duplicates_from_file(file_path, all_duplicates)
        print(f"  âœ… {removed_count}ä»¶ã®é‡è¤‡é …ç›®ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
    else:
        print("  âœ… é‡è¤‡é …ç›®ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== CIDãƒ»ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ é‡è¤‡æ’é™¤ ===")

    # å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
    base_dir = Path(r"c:\Users\HPE\Desktop\kueccha\sado-restaurant-map\tools\scraper\data\urls")
    target_files = [
        base_dir / "toilets_merged.txt",
        base_dir / "parkings_merged.txt"
    ]

    total_removed = 0

    for file_path in target_files:
        if file_path.exists():
            process_file(file_path)
        else:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")

    print(f"\nğŸ¯ å®Œäº†: é‡è¤‡æ’é™¤å‡¦ç†ãŒçµ‚äº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
