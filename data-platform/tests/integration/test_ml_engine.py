#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ML Engine çµ±åˆãƒ†ã‚¹ãƒˆ

ML Engineã®å…¨æ©Ÿèƒ½ã‚’åŒ…æ‹¬çš„ã«ãƒ†ã‚¹ãƒˆã—ã€å‹•ä½œç¢ºèªã‚’è¡Œã†çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚
ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æã€ç•°å¸¸æ¤œçŸ¥ã€å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãªã©ã®é«˜åº¦ãªMLæ©Ÿèƒ½ã‚’æ¤œè¨¼ã€‚
"""

import sys
import os
import logging
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
import tempfile
import shutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from shared.ml_engine import (
        MLEngine, QualityMetrics, MLPrediction, AnomalyReport,
        create_ml_engine
    )
except ImportError as e:
    print(f"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MLEngineIntegrationTest:
    """ML Engineçµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.test_results = []
        self.temp_dir = None
        self.ml_engine = None

    def setup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            self.temp_dir = tempfile.mkdtemp(prefix="ml_engine_test_")
            logger.info(f"ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—: {self.temp_dir}")

            # ML EngineåˆæœŸåŒ–
            self.ml_engine = create_ml_engine(model_dir=self.temp_dir)

            return True

        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def cleanup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.temp_dir and os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info("ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        except Exception as e:
            logger.warning(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

    def generate_test_data(self) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        test_data = [
            # é«˜å“è³ªãƒ‡ãƒ¼ã‚¿
            {
                "place_id": "ChIJ123456789",
                "name": "ä½æ¸¡ã®éƒ·åœŸæ–™ç†ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³",
                "formatted_address": "æ–°æ½ŸçœŒä½æ¸¡å¸‚ä¸¡æ´¥æ¹Š1-1",
                "rating": 4.2,
                "user_ratings_total": 85,
                "price_level": 2,
                "phone_number": "+81-259-12-3456",
                "website": "https://example-restaurant.com",
                "opening_hours": {
                    "weekday_text": ["æœˆæ›œæ—¥: 11:00ï½21:00", "ç«æ›œæ—¥: 11:00ï½21:00"]
                },
                "updated_at": datetime.now().isoformat()
            },
            # ä¸­å“è³ªãƒ‡ãƒ¼ã‚¿
            {
                "place_id": "ChIJ987654321",
                "name": "æµ·é®®å±…é…’å±‹",
                "formatted_address": "æ–°æ½ŸçœŒä½æ¸¡å¸‚ç›¸å·ä¸‹æˆ¸ç”º1-2",
                "rating": 3.8,
                "user_ratings_total": 42,
                "updated_at": (datetime.now() - timedelta(hours=12)).isoformat()
            },
            # ä½å“è³ªãƒ‡ãƒ¼ã‚¿ï¼ˆä¸å®Œå…¨ï¼‰
            {
                "place_id": "ChIJ111222333",
                "name": "test",
                "rating": 6.0,  # ç„¡åŠ¹ãªè©•ä¾¡å€¤
                "updated_at": (datetime.now() - timedelta(days=30)).isoformat()
            },
            # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿
            {
                "place_id": "ChIJ444555666",
                "name": "æ±äº¬ã®å¯¿å¸åº—",  # åœ°ç†çš„ä¸æ•´åˆ
                "formatted_address": "æ±äº¬éƒ½æ¸‹è°·åŒº1-1-1",
                "rating": 4.9,
                "user_ratings_total": 1,  # é«˜è©•ä¾¡ã ãŒè©•ä¾¡æ•°ãŒæ¥µå°
                "updated_at": datetime.now().isoformat()
            },
            # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ãƒ«æƒ…å ±ï¼‰
            {
                "place_id": "ChIJ777888999",
                "name": "ä½æ¸¡é‡‘å±±é£Ÿå ‚",
                "formatted_address": "æ–°æ½ŸçœŒä½æ¸¡å¸‚ç›¸å·é‡‘å±±ç”º1-3",
                "rating": 4.1,
                "user_ratings_total": 156,
                "price_level": 1,
                "phone_number": "+81-259-74-1234",
                "website": "https://sado-kinzan-restaurant.jp",
                "opening_hours": {
                    "weekday_text": ["æœˆæ›œæ—¥: 10:00ï½19:00", "ç«æ›œæ—¥: 10:00ï½19:00"],
                    "open_now": True
                },
                "updated_at": datetime.now().isoformat()
            }
        ]

        return test_data

    def generate_processing_history(self) -> List[Dict[str, Any]]:
        """å‡¦ç†å±¥æ­´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        history = []
        base_time = datetime.now() - timedelta(days=7)

        for i in range(50):
            job_time = base_time + timedelta(hours=i * 2)

            # æˆåŠŸã‚¸ãƒ§ãƒ–
            if i % 10 != 9:  # 90%æˆåŠŸç‡
                history.append({
                    "job_id": f"job_{i:03d}",
                    "status": "success",
                    "start_time": job_time.isoformat(),
                    "end_time": (job_time + timedelta(minutes=30 + i % 60)).isoformat(),
                    "duration": 30 + i % 60,
                    "data_count": 100 + i * 10,
                    "batch_size": 50,
                    "worker_count": 4,
                    "complexity": 0.3 + (i % 10) * 0.07
                })
            else:
                # å¤±æ•—ã‚¸ãƒ§ãƒ–
                history.append({
                    "job_id": f"job_{i:03d}",
                    "status": "failed",
                    "start_time": job_time.isoformat(),
                    "error": "Connection timeout",
                    "data_count": 50 + i * 5,
                    "worker_count": 4
                })

        return history

    def test_data_quality_analysis(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æãƒ†ã‚¹ãƒˆ"""
        test_name = "ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            test_data = self.generate_test_data()
            quality_metrics = self.ml_engine.analyze_data_quality(test_data)

            # åŸºæœ¬æ¤œè¨¼
            assert len(quality_metrics) == len(test_data), "å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°ãŒä¸€è‡´ã—ãªã„"
            assert all(isinstance(m, QualityMetrics) for m in quality_metrics), "QualityMetricsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å‹ä¸æ­£"

            # ã‚¹ã‚³ã‚¢ç¯„å›²æ¤œè¨¼
            for metric in quality_metrics:
                assert 0.0 <= metric.overall_score <= 1.0, f"ç·åˆã‚¹ã‚³ã‚¢ç¯„å›²å¤–: {metric.overall_score}"
                assert 0.0 <= metric.completeness <= 1.0, f"å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ç¯„å›²å¤–: {metric.completeness}"
                assert 0.0 <= metric.accuracy <= 1.0, f"æ­£ç¢ºæ€§ã‚¹ã‚³ã‚¢ç¯„å›²å¤–: {metric.accuracy}"
                assert 0.0 <= metric.consistency <= 1.0, f"ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ç¯„å›²å¤–: {metric.consistency}"
                assert 0.0 <= metric.confidence <= 1.0, f"ä¿¡é ¼åº¦ç¯„å›²å¤–: {metric.confidence}"

            # å“è³ªåˆ¤å®šæ¤œè¨¼
            high_quality_count = sum(1 for m in quality_metrics if m.overall_score >= 0.8)
            low_quality_count = sum(1 for m in quality_metrics if m.overall_score < 0.5)

            logger.info(f"å“è³ªåˆ†æçµæœ: é«˜å“è³ª={high_quality_count}ä»¶, ä½å“è³ª={low_quality_count}ä»¶")

            return {
                "status": "success",
                "metrics_count": len(quality_metrics),
                "high_quality_count": high_quality_count,
                "low_quality_count": low_quality_count,
                "average_score": sum(m.overall_score for m in quality_metrics) / len(quality_metrics)
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def test_anomaly_detection(self) -> Dict[str, Any]:
        """ç•°å¸¸æ¤œçŸ¥ãƒ†ã‚¹ãƒˆ"""
        test_name = "ç•°å¸¸æ¤œçŸ¥"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            test_data = self.generate_test_data()
            anomaly_reports = self.ml_engine.detect_anomalies(test_data)

            # åŸºæœ¬æ¤œè¨¼
            assert len(anomaly_reports) == len(test_data), "ç•°å¸¸ãƒ¬ãƒãƒ¼ãƒˆæ•°ãŒä¸€è‡´ã—ãªã„"
            assert all(isinstance(r, AnomalyReport) for r in anomaly_reports), "AnomalyReportã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å‹ä¸æ­£"

            # ç•°å¸¸æ¤œçŸ¥æ¤œè¨¼
            anomaly_count = sum(1 for r in anomaly_reports if r.is_anomaly)
            high_severity_count = sum(1 for r in anomaly_reports if r.severity == "high")

            # ã‚¹ã‚³ã‚¢ç¯„å›²æ¤œè¨¼
            for report in anomaly_reports:
                assert 0.0 <= report.anomaly_score <= 1.0, f"ç•°å¸¸ã‚¹ã‚³ã‚¢ç¯„å›²å¤–: {report.anomaly_score}"
                assert 0.0 <= report.confidence <= 1.0, f"ä¿¡é ¼åº¦ç¯„å›²å¤–: {report.confidence}"
                assert report.severity in ["none", "low", "medium", "high"], f"ç„¡åŠ¹ãªé‡è¦åº¦: {report.severity}"

            logger.info(f"ç•°å¸¸æ¤œçŸ¥çµæœ: ç•°å¸¸={anomaly_count}ä»¶, é«˜é‡è¦åº¦={high_severity_count}ä»¶")

            return {
                "status": "success",
                "total_reports": len(anomaly_reports),
                "anomaly_count": anomaly_count,
                "high_severity_count": high_severity_count
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def test_processing_time_prediction(self) -> Dict[str, Any]:
        """å‡¦ç†æ™‚é–“äºˆæ¸¬ãƒ†ã‚¹ãƒˆ"""
        test_name = "å‡¦ç†æ™‚é–“äºˆæ¸¬"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            # è¤‡æ•°ã®ã‚·ãƒŠãƒªã‚ªã§ãƒ†ã‚¹ãƒˆ
            test_scenarios = [
                {"query_count": 100, "complexity": 0.3, "workers": 4},
                {"query_count": 500, "complexity": 0.7, "workers": 8},
                {"query_count": 50, "complexity": 0.5, "workers": 2},
                {"query_count": 1000, "complexity": 0.9, "workers": 1}
            ]

            predictions = []
            for scenario in test_scenarios:
                prediction = self.ml_engine.predict_processing_time(
                    scenario["query_count"],
                    scenario["complexity"],
                    scenario["workers"]
                )

                # åŸºæœ¬æ¤œè¨¼
                assert isinstance(prediction, MLPrediction), "MLPredictionã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å‹ä¸æ­£"
                assert prediction.prediction >= 0, f"äºˆæ¸¬æ™‚é–“ãŒè² æ•°: {prediction.prediction}"
                assert 0.0 <= prediction.confidence <= 1.0, f"ä¿¡é ¼åº¦ç¯„å›²å¤–: {prediction.confidence}"
                assert prediction.explanation, "èª¬æ˜ãŒç©º"

                predictions.append(prediction)

            logger.info(f"å‡¦ç†æ™‚é–“äºˆæ¸¬ãƒ†ã‚¹ãƒˆå®Œäº†: {len(predictions)}ã‚·ãƒŠãƒªã‚ª")

            return {
                "status": "success",
                "scenarios_tested": len(predictions),
                "predictions": [
                    {
                        "time": p.prediction,
                        "confidence": p.confidence,
                        "explanation": p.explanation
                    } for p in predictions
                ]
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def test_batch_size_optimization(self) -> Dict[str, Any]:
        """ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        test_name = "ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            # è¤‡æ•°ã®ã‚·ãƒŠãƒªã‚ªã§ãƒ†ã‚¹ãƒˆ
            test_scenarios = [
                {"total": 1000, "workers": 8, "target": 300},
                {"total": 200, "workers": 2, "target": 600},
                {"total": 5000, "workers": 16, "target": 180},
                {"total": 50, "workers": 1, "target": 120}
            ]

            optimizations = []
            for scenario in test_scenarios:
                result = self.ml_engine.optimize_batch_size(
                    scenario["total"],
                    scenario["workers"],
                    scenario["target"]
                )

                # åŸºæœ¬æ¤œè¨¼
                assert "optimal_batch_size" in result, "æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒä¸æ˜"
                assert "estimated_batches" in result, "æ¨å®šãƒãƒƒãƒæ•°ãŒä¸æ˜"
                assert "confidence" in result, "ä¿¡é ¼åº¦ãŒä¸æ˜"

                batch_size = result["optimal_batch_size"]
                assert 1 <= batch_size <= scenario["total"], f"ãƒãƒƒãƒã‚µã‚¤ã‚ºç¯„å›²å¤–: {batch_size}"
                assert 0.0 <= result["confidence"] <= 1.0, f"ä¿¡é ¼åº¦ç¯„å›²å¤–: {result['confidence']}"

                optimizations.append(result)

            logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–ãƒ†ã‚¹ãƒˆå®Œäº†: {len(optimizations)}ã‚·ãƒŠãƒªã‚ª")

            return {
                "status": "success",
                "scenarios_tested": len(optimizations),
                "optimizations": optimizations
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def test_processing_pattern_analysis(self) -> Dict[str, Any]:
        """å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ†ã‚¹ãƒˆ"""
        test_name = "å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            processing_history = self.generate_processing_history()
            analysis_result = self.ml_engine.analyze_processing_patterns(processing_history)

            # åŸºæœ¬æ¤œè¨¼
            assert analysis_result["status"] == "success", f"åˆ†æå¤±æ•—: {analysis_result}"
            assert "patterns" in analysis_result, "ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ãŒä¸æ˜"
            assert "recommendations" in analysis_result, "æ¨å¥¨äº‹é …ãŒä¸æ˜"

            patterns = analysis_result["patterns"]

            # å¿…é ˆãƒ‘ã‚¿ãƒ¼ãƒ³è¦ç´ ã®æ¤œè¨¼
            required_patterns = [
                "basic_statistics", "temporal_patterns", "performance_patterns",
                "error_patterns", "efficiency_patterns", "predictions"
            ]

            for pattern_type in required_patterns:
                assert pattern_type in patterns, f"ãƒ‘ã‚¿ãƒ¼ãƒ³è¦ç´ ä¸è¶³: {pattern_type}"

            # çµ±è¨ˆæƒ…å ±ã®å¦¥å½“æ€§æ¤œè¨¼
            basic_stats = patterns["basic_statistics"]
            assert basic_stats["total_jobs"] == len(processing_history), "ã‚¸ãƒ§ãƒ–æ•°ä¸ä¸€è‡´"
            assert 0.0 <= basic_stats["success_rate"] <= 1.0, "æˆåŠŸç‡ç¯„å›²å¤–"

            logger.info(f"å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†: {basic_stats['total_jobs']}ã‚¸ãƒ§ãƒ–åˆ†æ")

            return {
                "status": "success",
                "jobs_analyzed": basic_stats["total_jobs"],
                "success_rate": basic_stats["success_rate"],
                "recommendations_count": len(analysis_result["recommendations"])
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def test_recommendation_generation(self) -> Dict[str, Any]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        test_name = "æ¨å¥¨äº‹é …ç”Ÿæˆ"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            test_data = self.generate_test_data()
            quality_metrics = self.ml_engine.analyze_data_quality(test_data)
            recommendations = self.ml_engine.generate_recommendations(quality_metrics)

            # åŸºæœ¬æ¤œè¨¼
            assert isinstance(recommendations, list), "æ¨å¥¨äº‹é …ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„"

            # æ¨å¥¨äº‹é …ã®æ§‹é€ æ¤œè¨¼
            for rec in recommendations:
                required_fields = ["type", "severity", "priority", "description", "actions"]
                for field in required_fields:
                    assert field in rec, f"æ¨å¥¨äº‹é …ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸è¶³: {field}"

                assert rec["severity"] in ["low", "medium", "high"], f"ç„¡åŠ¹ãªé‡è¦åº¦: {rec['severity']}"
                assert rec["priority"] in ["low", "medium", "high"], f"ç„¡åŠ¹ãªå„ªå…ˆåº¦: {rec['priority']}"
                assert isinstance(rec["actions"], list), "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒãƒªã‚¹ãƒˆå‹ã§ãªã„"

            logger.info(f"æ¨å¥¨äº‹é …ç”Ÿæˆãƒ†ã‚¹ãƒˆå®Œäº†: {len(recommendations)}ä»¶ç”Ÿæˆ")

            return {
                "status": "success",
                "recommendations_count": len(recommendations),
                "high_priority_count": sum(1 for r in recommendations if r["priority"] == "high")
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def test_status_monitoring(self) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç›£è¦–ãƒ†ã‚¹ãƒˆ"""
        test_name = "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç›£è¦–"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            status = self.ml_engine.get_comprehensive_status()

            # åŸºæœ¬æ¤œè¨¼
            assert "engine_info" in status, "ã‚¨ãƒ³ã‚¸ãƒ³æƒ…å ±ãŒä¸æ˜"
            assert "model_status" in status, "ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ãŒä¸æ˜"
            assert "configuration" in status, "è¨­å®šæƒ…å ±ãŒä¸æ˜"
            assert "statistics" in status, "çµ±è¨ˆæƒ…å ±ãŒä¸æ˜"

            engine_info = status["engine_info"]
            assert "version" in engine_info, "ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ãŒä¸æ˜"
            assert "sklearn_available" in engine_info, "sklearnå¯ç”¨æ€§ãŒä¸æ˜"
            assert "models_loaded" in engine_info, "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿çŠ¶æ…‹ãŒä¸æ˜"

            logger.info(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç›£è¦–ãƒ†ã‚¹ãƒˆå®Œäº†: v{engine_info.get('version', 'unknown')}")

            return {
                "status": "success",
                "engine_version": engine_info.get("version"),
                "sklearn_available": engine_info.get("sklearn_available"),
                "models_loaded": engine_info.get("models_loaded")
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def test_error_handling(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        test_name = "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"
        logger.info(f"{test_name}ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            error_tests = []

            # ç©ºãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ
            try:
                quality_metrics = self.ml_engine.analyze_data_quality([])
                assert quality_metrics == [], "ç©ºãƒ‡ãƒ¼ã‚¿å‡¦ç†å¤±æ•—"
                error_tests.append({"test": "empty_data", "status": "passed"})
            except Exception as e:
                error_tests.append({"test": "empty_data", "status": "failed", "error": str(e)})

            # ä¸æ­£ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ
            try:
                invalid_data = [{"invalid": "data"}]
                quality_metrics = self.ml_engine.analyze_data_quality(invalid_data)
                assert len(quality_metrics) == 1, "ä¸æ­£ãƒ‡ãƒ¼ã‚¿å‡¦ç†å¤±æ•—"
                error_tests.append({"test": "invalid_data", "status": "passed"})
            except Exception as e:
                error_tests.append({"test": "invalid_data", "status": "failed", "error": str(e)})

            # æ¥µç«¯å€¤ãƒ†ã‚¹ãƒˆ
            try:
                extreme_prediction = self.ml_engine.predict_processing_time(0, 0.0, 1)
                assert isinstance(extreme_prediction, MLPrediction), "æ¥µç«¯å€¤å‡¦ç†å¤±æ•—"
                error_tests.append({"test": "extreme_values", "status": "passed"})
            except Exception as e:
                error_tests.append({"test": "extreme_values", "status": "failed", "error": str(e)})

            passed_tests = sum(1 for test in error_tests if test["status"] == "passed")
            logger.info(f"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†: {passed_tests}/{len(error_tests)}ä»¶æˆåŠŸ")

            return {
                "status": "success",
                "total_tests": len(error_tests),
                "passed_tests": passed_tests,
                "test_results": error_tests
            }

        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "failed", "error": str(e)}

    def run_comprehensive_test(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("=== ML Engine çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        start_time = time.time()

        # ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        if not self.setup_test_environment():
            return {"status": "setup_failed"}

        try:
            # å„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            test_methods = [
                ("ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ", self.test_data_quality_analysis),
                ("ç•°å¸¸æ¤œçŸ¥", self.test_anomaly_detection),
                ("å‡¦ç†æ™‚é–“äºˆæ¸¬", self.test_processing_time_prediction),
                ("ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–", self.test_batch_size_optimization),
                ("å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ", self.test_processing_pattern_analysis),
                ("æ¨å¥¨äº‹é …ç”Ÿæˆ", self.test_recommendation_generation),
                ("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç›£è¦–", self.test_status_monitoring),
                ("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°", self.test_error_handling)
            ]

            test_results = {}
            passed_count = 0

            for test_name, test_method in test_methods:
                logger.info(f"--- {test_name}ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ ---")
                try:
                    result = test_method()
                    test_results[test_name] = result

                    if result.get("status") == "success":
                        passed_count += 1
                        logger.info(f"âœ… {test_name}ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
                    else:
                        logger.error(f"âŒ {test_name}ãƒ†ã‚¹ãƒˆ: å¤±æ•— - {result.get('error', 'Unknown error')}")

                except Exception as e:
                    error_msg = f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"
                    logger.error(f"âŒ {test_name}ãƒ†ã‚¹ãƒˆ: {error_msg}")
                    test_results[test_name] = {"status": "error", "error": error_msg}

            # ç·åˆçµæœ
            total_tests = len(test_methods)
            success_rate = passed_count / total_tests
            elapsed_time = time.time() - start_time

            comprehensive_result = {
                "status": "completed",
                "summary": {
                    "total_tests": total_tests,
                    "passed_tests": passed_count,
                    "failed_tests": total_tests - passed_count,
                    "success_rate": success_rate,
                    "elapsed_time": elapsed_time
                },
                "test_results": test_results,
                "timestamp": datetime.now().isoformat()
            }

            # çµæœã‚µãƒãƒªãƒ¼å‡ºåŠ›
            logger.info("=== ML Engine çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº† ===")
            logger.info(f"å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f}ç§’")
            logger.info(f"ãƒ†ã‚¹ãƒˆçµæœ: {passed_count}/{total_tests}ä»¶æˆåŠŸ ({success_rate:.1%})")

            if success_rate >= 0.8:
                logger.info("ğŸ‰ çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ: ML Engineã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
            elif success_rate >= 0.6:
                logger.warning("âš ï¸ çµ±åˆãƒ†ã‚¹ãƒˆéƒ¨åˆ†æˆåŠŸ: ä¸€éƒ¨æ©Ÿèƒ½ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
            else:
                logger.error("ğŸš¨ çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: é‡å¤§ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")

            return comprehensive_result

        finally:
            self.cleanup_test_environment()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ML Engine çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)

    # ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆã¨å®Ÿè¡Œ
    test_runner = MLEngineIntegrationTest()

    try:
        result = test_runner.run_comprehensive_test()

        # çµæœã®è©³ç´°å‡ºåŠ›
        print("\n" + "=" * 60)
        print("çµ±åˆãƒ†ã‚¹ãƒˆçµæœè©³ç´°:")
        print("=" * 60)

        summary = result.get("summary", {})
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {summary.get('total_tests', 0)}")
        print(f"æˆåŠŸãƒ†ã‚¹ãƒˆ: {summary.get('passed_tests', 0)}")
        print(f"å¤±æ•—ãƒ†ã‚¹ãƒˆ: {summary.get('failed_tests', 0)}")
        print(f"æˆåŠŸç‡: {summary.get('success_rate', 0):.1%}")
        print(f"å®Ÿè¡Œæ™‚é–“: {summary.get('elapsed_time', 0):.2f}ç§’")

        # è©³ç´°çµæœå‡ºåŠ›
        print("\nå€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ:")
        for test_name, test_result in result.get("test_results", {}).items():
            status_icon = "âœ…" if test_result.get("status") == "success" else "âŒ"
            print(f"{status_icon} {test_name}: {test_result.get('status', 'unknown')}")

            if test_result.get("status") != "success" and "error" in test_result:
                print(f"   ã‚¨ãƒ©ãƒ¼: {test_result['error']}")

        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        result_file = Path("ml_engine_integration_test_result.json")
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nè©³ç´°çµæœã‚’ä¿å­˜: {result_file}")

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
        success_rate = summary.get('success_rate', 0)
        exit_code = 0 if success_rate >= 0.8 else 1

        return exit_code

    except Exception as e:
        print(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
